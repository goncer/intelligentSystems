---
title: "Gonzalo Del Cerro Morera - NER - 3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Corpus annotation
```{r run, include = TRUE}

getwd()
setwd("/home/gonzo/workspace/R/intelligent/intelligentSystems/unit2")
##LoadWorkspace (to avoid waiitng times)
load("~/workspace/R/intelligent/intelligentSystems/unit2/afterLoadFiles.RData")
# Needed for OutOfMemoryError: Java heap space 
library(rJava)
.jinit(parameters="-Xmx4g")
# If there are more memory problems, invoke gc() after the POS tagging


library(openNLP) 
library(openNLPmodels.en)
library(tm)
library(stringr)

getAnnotationsFromDocument = function(doc){
  x=as.String(doc)
  sent_token_annotator <- Maxent_Sent_Token_Annotator()
  word_token_annotator <- Maxent_Word_Token_Annotator()
  pos_tag_annotator <- Maxent_POS_Tag_Annotator()
  y1 <- annotate(x, list(sent_token_annotator, word_token_annotator))
  y2 <- annotate(x, pos_tag_annotator, y1)
  #  parse_annotator <- Parse_Annotator()
  #  y3 <- annotate(x, parse_annotator, y2)
  return(y2)  
} 
getAnnotatedMergedDocument = function(doc,annotations){
  x=as.String(doc)
  y2w <- subset(annotations, type == "word")
  tags <- sapply(y2w$features, '[[', "POS")
  r1 <- sprintf("%s/%s", x[y2w], tags)
  r2 <- paste(r1, collapse = " ")
  return(r2)  
} 
getAnnotatedPlainTextDocument = function(doc,annotations){
  x=as.String(doc)
  a = AnnotatedPlainTextDocument(x,annotations)
  return(a)  
} 

detectPatternOnDocument <- function(doc, pattern) {
  x=as.String(doc)
  res=str_match(x,pattern)
  
  if (length(res)==1){
    return (res)
  } else {
    if (all(is.na(res[,2:length(res)])))
      return (NA)
    else {
      ret=list()
      for (i in 2:length(res)){
        ret = paste(ret,res[i])
      }
      return(ret)
    }
  }
}

detectPatternOnDocumentWithContext <- function(doc, pattern) {
  txt=as.String(doc)
  number=50
  coord=str_locate(txt,pattern)
  res3=substr(txt,coord[1]-number,coord[2]+number)
  return (res3)
}
detectPatternsInCorpus = function(corpus, patterns){
  vallEntities <- data.frame(matrix(NA, ncol = length(patterns)+1, 
                                    nrow = length(corpus)))
  names(vallEntities) <- c("File",patterns)
  for (i in 1:length(patterns)) {
    vallEntities[,i+1]=unlist(lapply(corpus, detectPatternOnDocument, 
                                     pattern=patterns[i]))
  }
  for (i in 1:length(corpus)) {
    vallEntities$File[i]=meta(corpus[[i]])$id
  }
  return (vallEntities)  
}

detectPatternsInTaggedCorpus = function(corpus, taggedCorpus, patterns){
  vallEntities <- data.frame(matrix(NA, ncol = length(patterns)+1, 
                                    nrow = length(corpus)))
  names(vallEntities) <- c("File",patterns)
  for (i in 1:length(patterns)) {
    vallEntities[,i+1]=unlist(lapply(taggedCorpus, detectPatternOnDocument, 
                                     pattern=patterns[i]))
  }
  for (i in 1:length(corpus)) {
    vallEntities$File[i]=meta(corpus[[i]])$id
  }
  return (vallEntities)  
}

countMatchesPerColumn = function (df) {
  entityCountPerPattern <- data.frame(matrix(NA, ncol = 2, 
                                             nrow = length(names(df))-1))
  names(entityCountPerPattern) <- c("Entity","Count")
  
  for (i in 2:length(names(df))) {
    entityCountPerPattern$Entity[i-1] = names(df)[i]
    entityCountPerPattern$Count[i-1] = nrow(subset(df, !is.na(df[i])))
  }
  return (entityCountPerPattern)
}
countMatchesPerRow = function (df) {
  entityCountPerFile <- data.frame(matrix(NA, ncol = 2, nrow = nrow(df)))
  names(entityCountPerFile) <- c("File","Count")
  
  for (i in 1:nrow(df)) {
    entityCountPerFile$File[i] = df$File[i]
    entityCountPerFile$Count[i] = length(Filter(Negate(is.na),df[i,2:length(df[i,])]))
  }
  return (entityCountPerFile[entityCountPerFile[2]!=0,])
}

printMatchesPerPattern = function (patterns, matches) {
  for (i in 1:length(patterns)){
    print(paste("PATTERN: ",patterns[i]))
    strings = matches[,i+1][!is.na(unlist(matches[,i+1]))]
    print(strings)
    print(" ") 
  }
}

mergeAllMatchesInLists = function (df) {
  matchesPerFile = rep(list(list()), nrow(df))
  
  for (i in 1:nrow(df)) {    
    matches=as.list(unname(unlist(Filter(Negate(is.na),df[i,2:length(df[i,])]))))
    matchesPerFile[[i]]=append(matchesPerFile[[i]],matches)
  }
  
  files = df[,1]
  matches = matchesPerFile
  
  allMatches<- data.frame(matrix(NA, ncol = 2, nrow = nrow(df)))
  names(allMatches) <- c("Files","Matches")
  
  allMatches$Files=files
  allMatches$Matches=matches
  
  return (allMatches)
}

mergeGoldStandardInLists = function (df) {
  matchesPerFile = rep(list(list()), nrow(df))
  
  for (i in 1:nrow(df)) {    
    matches=as.list(unlist(Filter(Negate(is.na),df[i,2:length(df)])))
    matchesPerFile[[i]]=append(matchesPerFile[[i]],matches)
  }
  
  files = df[,1]
  matches = matchesPerFile
  
  allMatches<- data.frame(matrix(NA, ncol = 2, nrow = nrow(df)))
  names(allMatches) <- c("Files","Matches")
  
  allMatches$Files=files
  allMatches$Matches=matches
  
  return (allMatches)
}

calculateMetrics = function (matches, matches.gs, seeDifferences = FALSE) {
  
  metrics<- data.frame(matrix(NA, ncol = 3, nrow = 1))
  names(metrics) <- c("Precision","Recall","Fmeasure")
  
  numCorrect = 0
  allAnswers = 0
  possibleAnswers = 0
  
  for (i in 1:nrow(matches)) {    
    if (length(matches.gs$Matches[[i]])!=0) {
      l = str_trim(unlist(matches[i,2]))
      l.gs = unname(unlist(matches.gs[i,2]))
      intersection = intersect(l, l.gs)
      numCorrect = numCorrect + length(intersect(l, l.gs))
      allAnswers = allAnswers + length (l)
      possibleAnswers = possibleAnswers + length(l.gs)
      #GonCer:To check wich names doe snot mach, place for improvements,new regex to be created!.
      if(seeDifferences)
        print(paste("differences in: " , setdiff(l, l.gs),setdiff(l.gs, l )))
    }
   
  }
  
  metrics$Precision = numCorrect / allAnswers
  metrics$Recall = numCorrect / possibleAnswers
  
  beta = 1
  metrics$Fmeasure= ((sqrt(beta)+1) * metrics$Precision * metrics$Recall) / 
    ((sqrt(beta)*metrics$Precision) + metrics$Recall)
  
  return(metrics)
}

source.pos = DirSource("./txt_sentoken/pos", encoding = "UTF-8")
#source.pos = DirSource("./txt_sentoken/pos",
#                       encoding = "UTF-8", pattern="(cv42[0-9]_[0-9]+.\\w*)")

corpus = Corpus(source.pos)

#As if it loaded in the first sentence, we dont need to generate it again ow, uncomment the following line.
#annotations = lapply(corpus, getAnnotationsFromDocument)
#annotations[[1]]

corpus.tagged = Map(getAnnotatedPlainTextDocument, corpus, annotations)
corpus.taggedText = Map(getAnnotatedMergedDocument, corpus, annotations)

goldStandard = read.table(file = "./goldStandard.csv", quote = "", na.strings=c(""),
                          colClasses="character", sep=";")


#simple by topc.

pattern0=c("created by")
pattern0=c(pattern0,"screenwriter[s]?")
pattern0=c(pattern0,"cinematographer")
pattern0=c(pattern0,"oscar winner")
matches0 = detectPatternsInCorpus(corpus, pattern0)
#matches0[!is.na(matches0[3]),c(1,3)]
entityCountPerPattern = countMatchesPerRow(matches0) 
countMatchesPerColumn(matches0) 

##for (i in 1:length(pattern0)){
##  print(paste("PATTERN: ",pattern0[i]))
##  strings = lapply(corpus, detectPatternOnDocumentWithContext, pattern=pattern0[i])
##  print(unlist(strings[!is.na(unlist(strings))]))
##  print(" ")
##}
#write.table(matches0, file = "matches0.csv", row.names = F, na="", sep=";")

matches0 = mergeAllMatchesInLists(matches0)
head(matches0)


allMatchesGold = mergeGoldStandardInLists(goldStandard)
#head(allMatchesGold)
metrics0 = calculateMetrics(matches0, allMatchesGold)
metrics0

##FindEntitites:
pattern1=c("created by ([A-z]* [A-z]*)")
pattern1=c(pattern1,"created by [A-z]* [A-z]* \\( and ([A-z]* [A-z]*)")
pattern1=c(pattern1,"screenwriter[s]? ([A-z]* [A-z]*)")
pattern1=c(pattern1,"cinematographer(?: ,)? ([A-z]* [A-z]*)")
pattern1=c(pattern1,"oscar winner ([A-z]* [A-z]*)")
pattern1=c(pattern1,"co-star [A-z]* [A-z]*")
pattern1=c(pattern1,"co-stars [A-z]* [A-z]* \\( and ([A-z]* [A-z]*)")

matches1 = detectPatternsInCorpus(corpus, pattern1)
#matches1[!is.na(matches1[4]),c(1,4)]
printMatchesPerPattern(pattern1, matches1)
entityCountPerPattern =countMatchesPerRow(matches1) 
countMatchesPerColumn(matches1) 

write.table(matches1, file = "matches1.csv", row.names = F, na="", sep=";")
matches1 = mergeAllMatchesInLists(matches1)
head(matches1)

allMatchesGold = mergeGoldStandardInLists(goldStandard)
#head(allMatchesGold)
metrics1 = calculateMetrics(matches1, allMatchesGold)


allMatchesGold = mergeGoldStandardInLists(goldStandard)

##POS Taags
pattern2 = ""
pattern2=c("created/VBN by/IN ([A-z]*)/NN ([A-z]*)/NN")
pattern2=c(pattern2,"created/VBN by/IN [A-z]*/NN [A-z]*/NN \\(/-LRB- and/CC ([A-z]*)/JJ ([A-z]*)/NN")
pattern2=c(pattern2,"screenwriter[s]?/NN[S]? ([A-z]*)/(?:NN[S]?|JJ) ([A-z]*)/(?:NN|JJ)")
pattern2=c(pattern2,"cinematographer/NN(?: ,/,)? ([A-z]*)/NN ([A-z]*)/NN")
pattern2=c(pattern2,"cinematographer/NN(?: ,/,)? ([A-z]*)/NN ([A-z]*)/IN ([A-z]*)/NN")
pattern2=c(pattern2,"oscar/NN winner/NN ([A-z]*)/VBG ([A-z]*)/NNS")



allEntities = detectPatternsInTaggedCorpus(corpus, corpus.taggedText, pattern2)
allMatches = mergeAllMatchesInLists(allEntities)
metrics = calculateMetrics(allMatches, allMatchesGold,FALSE)

metrics0
metrics1
metrics
```


## Which patterns have worked better? Which worse?
As we can see, the first patterns, without the tagging works worst, Besides the Recall is higher in the non-tagged search, only searching by regular expression is not enough,the precision decreases dramatically and the Fmseasure increase is not relevant enough.   

For increasing it a bit, we will try to add more Reg-ex over the POS tags patterns and avoid using only Reg-ex search.

##Recognize entities 

```{r improvements}

##extra tags:
pattern2=c(pattern2,"co-star/JJ ([A-z]*)/NN ([A-z]*)/NN")
allEntities = detectPatternsInTaggedCorpus(corpus, corpus.taggedText, pattern2)
allMatches = mergeAllMatchesInLists(allEntities)
metrics = calculateMetrics(allMatches, allMatchesGold,FALSE)
metrics

##co-stars/NNS matt/NN damon/NN
pattern2=c(pattern2, "co-stars/NNS [A-z]*/NN [A-z]*/NN and/CC ([A-z]*)/NN ([A-z]*)/NN")
pattern2=c(pattern2, "co-stars/NNS ([A-z]*)/NN ([A-z]*)/NN and/CC [A-z]*/NN [A-z]*/NN")

allEntities = detectPatternsInTaggedCorpus(corpus, corpus.taggedText, pattern2)
allMatches = mergeAllMatchesInLists(allEntities)
metrics = calculateMetrics(allMatches, allMatchesGold,FALSE)
metrics

##director/NN bruce/NN beresford/NN
pattern2=c(pattern2,"director/NN ([A-z]*)/NN ([A-z]*)/NN")

allEntities = detectPatternsInTaggedCorpus(corpus, corpus.taggedText, pattern2)
allMatches = mergeAllMatchesInLists(allEntities)
metrics = calculateMetrics(allMatches, allMatchesGold,FALSE)
metrics

```
##Assess approach

After including them, we can see that we increased the precision, the recall and F measure. The more frequent is the term searched and the more standard is the language, the better search result and more names we obtain from one regular expression.

##Conclussions and next steps

From my point of view, extracting names is a complex task when the sentences are not normalized. I would rather obtain some names of actors from external databases (such DBpedia), in order to increase the Recall and precision. However, the regular expressions created are not reusable for another domain, so I would give it a try on analyzing syntactically the phrases and comparing the /NN tagged words with names present on a DB of names and storing them as true positive.
